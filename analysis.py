import cv2
import numpy as np
from deepface import DeepFace
from insightface.app import FaceAnalysis
import tensorflow as tf
from collections import Counter
import os
import json

# --- 1. C·∫§U H√åNH ---
VIDEO_PATH = "MAH07303.mp4" # <--- ƒê∆Ø·ªúNG D·∫™N VIDEO ƒê·∫¶U V√ÄO
OUTPUT_JSON_PATH = "video_features.json"
MASK_MODEL_PATH = 'model-facemask.h5'

# Tham s·ªë ph√¢n t√≠ch
ANALYSIS_INTERVAL_SEC = 2
MASK_PROB_THRESHOLD = 0.7

# --- 2. T·∫¢I C√ÅC M√î H√åNH PH√ÇN T√çCH ---
print("ƒêang t·∫£i c√°c m√¥ h√¨nh ph√¢n t√≠ch (InsightFace, DeepFace, Mask)...")
try:
    # V·∫´n t·∫£i c√°c model n√†y b·∫±ng tf.keras ƒë·ªÉ xem l·ªói c√≥ x·∫£y ra ·ªü ƒë√¢y kh√¥ng
    app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
    app.prepare(ctx_id=0, det_size=(640, 640))
    mask_model = tf.keras.models.load_model(MASK_MODEL_PATH)
    print("‚úÖ C√°c m√¥ h√¨nh ph√¢n t√≠ch ƒë√£ t·∫£i th√†nh c√¥ng.")
except Exception as e:
    print(f"L·ªói khi t·∫£i m√¥ h√¨nh ph√¢n t√≠ch: {e}")
    exit()

# --- 3. PH√ÇN T√çCH VIDEO V√Ä TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG ---
print(f"\n‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu ph√¢n t√≠ch video '{VIDEO_PATH}' ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng...")
cap = cv2.VideoCapture(VIDEO_PATH)
fps = cap.get(cv2.CAP_PROP_FPS)
if fps == 0:
    print(f"L·ªói: Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c FPS t·ª´ video {VIDEO_PATH}")
    exit()

all_emotions_in_clip = []
frame_count = 0
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break

    if frame_count % int(fps * ANALYSIS_INTERVAL_SEC) == 0:
        try:
            faces = app.get(frame)
            for face in faces:
                bbox = face.bbox.astype(np.int32)
                face_img_cropped = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]
                if face_img_cropped.size == 0: continue
                
                face_resized = cv2.resize(face_img_cropped, (224, 224), interpolation=cv2.INTER_AREA)
                face_batch = np.expand_dims(face_resized / 255.0, axis=0)
                mask_prob = mask_model.predict(face_batch, verbose=0)[0][0]
                
                emotion = "masked"
                if mask_prob < MASK_PROB_THRESHOLD:
                    results = DeepFace.analyze(face_resized, actions=['emotion'], enforce_detection=False, silent=True)
                    emotion = results[0]['dominant_emotion']
                
                all_emotions_in_clip.append(emotion)
        except Exception:
            pass
    
    frame_count += 1
    print(f"\rƒê√£ x·ª≠ l√Ω frame: {frame_count}/{total_frames}", end="")

cap.release()
print("\n‚úÖ Ph√¢n t√≠ch v√† thu th·∫≠p d·ªØ li·ªáu ho√†n t·∫•t.")

# --- 4. T√çNH TO√ÅN V√Ä L∆ØU ƒê·∫∂C TR∆ØNG RA FILE JSON ---
print(f"\nüíæ ƒêang t√≠nh to√°n v√† l∆∞u ƒë·∫∑c tr∆∞ng v√†o file '{OUTPUT_JSON_PATH}'...")
output_data = {'video_path': VIDEO_PATH, 'features': {}}
if all_emotions_in_clip:
    valid_emotions = [e for e in all_emotions_in_clip if e not in ['masked', 'unknown', 'error']]
    emotion_counts = Counter(valid_emotions)
    total_valid = sum(emotion_counts.values())
    
    if total_valid > 0:
        possible_emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
        for emotion in sorted(possible_emotions):
            percent = (emotion_counts.get(emotion, 0) / total_valid * 100)
            output_data['features'][f"percent_{emotion}"] = percent

with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, indent=4)

print("üéâ Ho√†n t·∫•t! ƒê√£ t·∫°o file ƒë·∫∑c tr∆∞ng. B√¢y gi·ªù h√£y ch·∫°y file 'predict_from_json.py'.")